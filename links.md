###A Neural Network in 11 lines of Python
http://iamtrask.github.io/2015/07/12/basic-python-network/

A bare bones neural network implementation to describe the
inner workings of backpropagation.

Posted by [iamtrask] (https://iamtrask.github.io/) on July 12, 2015

---

###Recurrent Neural Networks Tutorial
http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/
- http://colah.github.io/posts/2015-08-Understanding-LSTMs/
- http://karpathy.github.io/2015/05/21/rnn-effectiveness/
- Optimizing SDG: http://cs231n.github.io/optimization-1/

---

###Understanding LSTM Networks
http://colah.github.io/posts/2015-08-Understanding-LSTMs/


https://larseidnes.com/2015/10/13/auto-generating-clickbait-with-recurrent-neural-networks/

---

###Math Stuff
https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent

```Here, the term "stochastic" comes from the fact that the gradient based on a single training sample is a "stochastic approximation" of the "true" cost gradient. Due to its stochastic nature, the path towards the global cost minimum is not "direct" as in GD, but may go "zig-zag" if we are visualizing the cost surface in a 2D space. However, it has been shown that SGD almost surely converges to the global cost minimum if the cost function is convex (or pseudo-convex)```

